{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent, the Normal Equation, Learning Rates, and why all of this matters for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is typically used as the hello world for data science. It's one of the oldest machine learning models (designed far before machines could learn) but, despite being so old, it's far from being the simplest model. Understanding linear regression is also the first step toward understanding more sophisticated models like logistic regression, support vector machines, and even deep learning models.\n",
    "\n",
    "Below I'll get into some of the nitty-gritty of linear regression so that those reading can have a deeper understanding of how the model works and, if you choose, create your own implementation of it.\n",
    "\n",
    "I don't see any point in waiting, so, let's dive into the code and do some necessary imports.\n",
    "\n",
    "We'll use `numpy` for speed, and `minmax_scale`, `train_test_split`, and `r2_score` from `sklearn` for convenience and `LinearRegression` from `sklearn` to test some of our implementations. We've also imported `time` and `altair` to compare the time complexity of one of our implementations against `sklearn`'s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from time import time\n",
    "import altair as alt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than direct everyone to a potentially broken link, feel free to test any implementations using the code below to generate data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data(n_feats=8, n_obs=1000):\n",
    "    features = ['f' + str(x) for x in range(n_feats)]\n",
    "    target = abs(np.random.normal(1000, 250, n_obs))\n",
    "    feat_vals = []\n",
    "    for feature in features:\n",
    "        feat_vals.append([np.random.normal(\n",
    "                            abs(np.random.normal(target[i],\n",
    "                            abs(np.random.normal(100, 30)))))\n",
    "                            for i in range(n_obs)])\n",
    "    X = np.array(feat_vals).T\n",
    "    return target, X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "target, X = create_data(n_feats=10, n_obs=10)\n",
    "X_scaled = minmax_scale(X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, target,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Line of Best Fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can't talk about machine learning models like Linear Regression without first quickly reviewing where these models come from.  \n",
    "\n",
    "The great grandfather of linear regression is the line of best fit. You use it when you have a single input variable and a single output variable. That's when it works.\n",
    "\n",
    "Given $n \\times 1$ explanatory and response variables, we can calculate the slope and intercept of the line of best fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8706799041962858"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train = X_train[:, 0]\n",
    "x_test = X_test[:, 0]\n",
    "\n",
    "x_mean = x_train.mean()\n",
    "y_mean = y_train.mean()\n",
    "m = ((x_train - x_mean) * (y_train - y_mean)).sum() / ((x_train - x_mean)**2).sum()\n",
    "b = y_mean - m * x_mean\n",
    "preds = m * x_test + b\n",
    "1 - ((y_test - preds)**2).sum() / ((y_test - y_mean)**2).sum()        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be done without iteration. We simply find the slope of the line (`m`) and then use that while calculating our intercept (`b`).  \n",
    "\n",
    "We can see that the prediction is (almost) the same as when we use sklearn's LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8622231033771592"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "LinearRegression().fit(x_train.reshape(-1, 1), y_train).score(x_test.reshape(-1, 1), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Equation Method\n",
    "The line of best fit works when we have a single explanatory variable. What do we do when we have multiple explanatory variables? For that we take a huge leap forward in complexity and use the normal equation; the equation that we get from differentiating the residual sum of sqaures equation.\n",
    "\n",
    "The derivative of RSS deserves it's entire own blog/video, fortunately it's already been done a million times over. Search for your own explanation or just look at this [video](https://www.youtube.com/watch?v=K_EH2abOp00) or at [wiki](https://en.wikipedia.org/wiki/Proofs_involving_ordinary_least_squares) for both the matrix and summation notation.\n",
    "\n",
    "Ultimately, when we differentiate the residual sum of squares equation:  \n",
    "$RSS={\\bigl \\|}\\mathbf {y} -\\mathbf {X}{\\beta }{\\bigl \\|}^2$  \n",
    "we get:  \n",
    "$(\\mathbf {X} ^{\\rm {T}}\\mathbf {X} )^{-1}\\mathbf {X} ^{\\rm {T}}\\mathbf {y} =0$  \n",
    "\n",
    "This is the equation that we'll use in our next example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalEquation:\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def __repr__(self):\n",
    "        return \"OLS()\"\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        # Our new column that we'll call intercept\n",
    "        # We make it based on the shape of the first column in our training data\n",
    "        intercept = np.ones_like(X_train[:, 0])\n",
    "        \n",
    "        # We add our new column to our training data\n",
    "        X_train_int = np.insert(X_train, 0, intercept, axis=1)\n",
    "        \n",
    "        # Use the normal equation\n",
    "        coefs = np.linalg.inv(X_train_int.T @ X_train_int) @ X_train_int.T @ y_train\n",
    "        \n",
    "        # Save our coefficients for later use\n",
    "        self.coefs_ = coefs\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X_test):\n",
    "        # Again, we create a new column for the test data\n",
    "        intercept = np.ones_like(X_test[:, 0])\n",
    "        X_test_int = np.insert(X_test, 0, intercept, axis=1)\n",
    "        preds = X_test_int @ self.coefs_\n",
    "        return preds\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        # Again, we create a new column for the test data\n",
    "        intercept = np.ones_like(X_test[:, 0])\n",
    "        X_test_int = np.insert(X_test, 0, intercept, axis=1)\n",
    "        preds = X_test_int @ self.coefs_\n",
    "        \n",
    "        # Here we're simply calculating r-squared\n",
    "        score = 1 - ((y_test - preds)**2).sum() / ((y_test - y_test.mean())**2).sum()\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67, 99)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NormalEquation().fit(X[:, :-1], target).score(X[:, :-1], target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinearRegression().fit(X_train, y_train).score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-42-093d6996c105>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m         \u001b[0mstart\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m         \u001b[0mLinearRegression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m         \u001b[0mend\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0msklearn_LinRegr_sub\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vv\\lib\\site-packages\\sklearn\\linear_model\\base.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    501\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    502\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_residues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrank_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msingular_\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 503\u001b[1;33m                 \u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlstsq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    504\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\envs\\vv\\lib\\site-packages\\scipy\\linalg\\basic.py\u001b[0m in \u001b[0;36mlstsq\u001b[1;34m(a, b, cond, overwrite_a, overwrite_b, check_finite, lapack_driver)\u001b[0m\n\u001b[0;32m   1209\u001b[0m                 \u001b[0mlwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miwork\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_compute_lwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlapack_lwork\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcond\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1210\u001b[0m                 x, s, rank, info = lapack_func(a1, b1, lwork,\n\u001b[1;32m-> 1211\u001b[1;33m                                                iwork, cond, False, False)\n\u001b[0m\u001b[0;32m   1212\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# complex data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1213\u001b[0m                 lwork, rwork, iwork = _compute_lwork(lapack_lwork, m, n,\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_observations = range(2_500, 100_001, 2_500)\n",
    "# n_observations = range(100, 2001, 100)\n",
    "\n",
    "i = 0\n",
    "\n",
    "normal_equation = []\n",
    "sklearn_LinRegr = []\n",
    "for n in n_observations:\n",
    "    y, X = create_data(n_feats=8, n_obs=n)\n",
    "    X = minmax_scale(X)\n",
    "    \n",
    "    normal_equation_sub = []\n",
    "    sklearn_LinRegr_sub = []\n",
    "    \n",
    "    for _ in range(5):\n",
    "    \n",
    "        start = time()\n",
    "        NormalEquation().fit(X, y)\n",
    "        end = time()\n",
    "        normal_equation_sub.append(end - start)\n",
    "\n",
    "        start = time()\n",
    "        LinearRegression().fit(X, y)\n",
    "        end = time()\n",
    "        sklearn_LinRegr_sub.append(end - start)\n",
    "        \n",
    "    normal_equation.append(sum(normal_equation_sub)/5)\n",
    "    sklearn_LinRegr.append(sum(sklearn_LinRegr_sub)/5)\n",
    "\n",
    "    print(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x = np.arange(100)\n",
    "source = pd.DataFrame({\n",
    "    'Duration': normal_equation + sklearn_LinRegr,\n",
    "    'Model':['Normal Equation'] * 40 + ['Sklearn'] * 40,\n",
    "    'N Observations': list(n_observations) * 2\n",
    "})\n",
    "\n",
    "alt.Chart(source).mark_line().encode(\n",
    "    x='N Observations',\n",
    "    y='Duration',\n",
    "    color='Model'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_feats = range(10, 101, 10)\n",
    "\n",
    "i = 0\n",
    "\n",
    "normal_equation = []\n",
    "sklearn_LinRegr = []\n",
    "for n in n_feats:\n",
    "    y, X = create_data(n_feats=n, n_obs=10_000)\n",
    "    X = minmax_scale(X)\n",
    "    \n",
    "    normal_equation_sub = []\n",
    "    sklearn_LinRegr_sub = []\n",
    "    \n",
    "    for _ in range(5):\n",
    "    \n",
    "        start = time()\n",
    "        NormalEquation().fit(X, y)\n",
    "        end = time()\n",
    "        normal_equation_sub.append(end - start)\n",
    "\n",
    "        start = time()\n",
    "        LinearRegression().fit(X, y)\n",
    "        end = time()\n",
    "        sklearn_LinRegr_sub.append(end - start)\n",
    "        \n",
    "    normal_equation.append(sum(normal_equation_sub)/5)\n",
    "    sklearn_LinRegr.append(sum(sklearn_LinRegr_sub)/5)\n",
    "\n",
    "    print(i)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "x = np.arange(100)\n",
    "source = pd.DataFrame({\n",
    "    'Duration': normal_equation + sklearn_LinRegr,\n",
    "    'Model':['Normal Equation'] * 10 + ['Sklearn'] * 10,\n",
    "    'N Features': list(n_feats) * 2\n",
    "})\n",
    "\n",
    "alt.Chart(source).mark_line().encode(\n",
    "    x='N Features',\n",
    "    y='Duration',\n",
    "    color='Model'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = create_data(n_feats=10_000, n_obs=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled = minmax_scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y,\n",
    "                                                    test_size=0.33,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-54643372.58079182"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne = NormalEquation()\n",
    "ne.fit(X_train, y_train)\n",
    "ne.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([495727.05459841, 317772.91850209, 274498.81326017, ...,\n",
       "        56785.84588652,  33635.2016939 ,  73501.73016676])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne.coefs_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-8981.175180256967"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ne.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=None, normalize=False)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.06580018,  0.192062  ,  0.12335203, ..., -0.10378055,\n",
       "        0.88402512,  0.25159258])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999448592509455"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %%timeit\n",
    "LinearRegression().fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we create an additional column of all ones in our training data and out test data. This will act as the intercept.  \n",
    "If you didn't add the column of ones, the equation would calculate all coefficients given an intercept of `0`. \n",
    "\n",
    "An exercise and thought experiment for you:  \n",
    "\n",
    "> Try timing this implementation against sklearn's LinearRegression with varying numbers of features and observations. How does time complexity scale? Why is this the case?\n",
    "\n",
    "> Think about how one would account for an intercept using the normal equation without adding the column of ones.  <br>\n",
    ">> Hint: what does the intercept do, and what might you do so that an intercept of `0` is the ideal intercept?\n",
    "\n",
    "Side note:  \n",
    "For anymore unfamiliar with the `__repr__` \"dunder\", try deleting it and finding out what changes. If you'd like to learn more, I strongly recommend reading [Fluent Python](http://index-of.es/Varios-2/Fluent%20Python%20Clear%20Concise%20and%20Effective%20Programming.pdf). In it, they go over `__repr__` and some of the other \"magic methods\" found in python and how they compare to those found in other programming languages (as well as pretty anything else you might even want to know about python)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the normal equation above is incredibly fast given the size of our data (for those who didn't try on their own, it's actually significantly faster than sklearn for a dataset of this size), it does not scale particularly well; that's why we typically use gradient descent in practice.\n",
    "\n",
    "Below I'll walk through taking the derivative of the cost function for gradient descent as well as provide some sudo code to warm us up before we dive into any more full fledged models.\n",
    "\n",
    "I'll walk through this derivative both becuase it is simpler than the derivative of RSS and it's what you'd likely be expected to code if you were asked to create a linear regression model in an interview. Knowing how to derive it on your own should help with remembering it for when it's necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differentiating Gradient Descent\n",
    "\n",
    "We start with a single squared error, or the _definition of the cost function_ which you can see below using two types of notation:  \n",
    "\n",
    "$ (y - (mx + b))^2 $\n",
    "\n",
    "and  \n",
    "$\\frac{1}{2m}\\sum_{i=1}^{m} \\left(h_{\\theta}(x^{(i)})-y^{(i)}\\right)^2$\n",
    "\n",
    "We'll use the first notation going forward.\n",
    "\n",
    "As mentioned, we need to take the derivative of the cost function. We will take it with respect to both $m$ (our coefficient) and $b$ (our intercept).\n",
    "\n",
    "#### Derivative with respect to $m$:  \n",
    "\n",
    "$\\frac{d}{dm}\\left[(y - (mx + b))^2\\right]$ ->  \n",
    "\n",
    "$2 \\times (y - (mx + b)) \\frac{d}{dm}\\left[(y - (mx + b))\\right]$ ->  \n",
    "\n",
    "$\\frac{d}{dm} 2 \\times (y - (mx + b)) \\times x$  \n",
    "\n",
    "We don't actually need the $2$ (we're minimizing this... it doesn't matter if we minimize $x$ or $2x$) so we just end up with:  \n",
    "\n",
    "$\\frac{d}{dm} (y - (mx + b)) \\times x$  \n",
    "\n",
    "#### Taking steps with $m$:  \n",
    "\n",
    "We multiply the derivative by the learning rate to get our step.  \n",
    "The code looks something like this:\n",
    "``` python\n",
    "pred = mx + b\n",
    "error = y - pred\n",
    "derivative = error * x\n",
    "step = derivative * learning_rate\n",
    "```\n",
    "After taking the derivative with respect to $m$, the derivative with respect to $b$ should be slightly easier:\n",
    "\n",
    "#### Derivative with respect to $b$:  \n",
    "\n",
    "$\\frac{d}{dm}\\left[(y - (mx + b))^2\\right]$ ->  \n",
    "\n",
    "$2 \\times (y - (mx + b)) \\frac{d}{dm}\\left[(y - (mx + b))\\right]$ ->  \n",
    "\n",
    "$\\frac{d}{dm} 2 \\times (y - (mx + b)) \\times 1$ ->\n",
    "\n",
    "$\\frac{d}{dm} y - (mx + b)$\n",
    "\n",
    "The only difference is that we're not multiplying $b$ by anything so it essentially dissappears after taking its derivative in the second stage of the chain.  \n",
    "\n",
    "#### Taking steps with $b$:  \n",
    "Now we take a look at the code (note how similar it is to the step for the coefficient, $m$):\n",
    "``` python\n",
    "pred = mx + b\n",
    "error = y - pred\n",
    "derivative = error\n",
    "step = derivative * learning_rate\n",
    "```\n",
    "\n",
    "### Combining what we've learned\n",
    "The step for the intercept ($b$) is just the error multiplied by the learning rate!  \n",
    "The last two lines of code can obviously be simplified to `step = error * learning_rate`.  \n",
    "\n",
    "This can be abstracted to matrix multiplication quite simply.  \n",
    "Using `numpy`, the `@` operator will act as a matrix multiplier (`numpy` also has `arr.dot(arr2)` but I haven't noticed any performance improvments and I find `@` more readable). We simply swap out the `*` with `@` wherever we're multiplying matrices (but don't forget that order matters during matrix multiplication!).  \n",
    "(_Note that due to matrix multiplication magic, this can work with a single observation, a batch of observations, or even an entire dataset._ You simply need to divide by the number of observations used in the calculation and multiply by the learning rate).\n",
    "``` python\n",
    "n = len(X[0])\n",
    "pred = (X @ m) + b\n",
    "error = (y - pred) * (1/n)\n",
    "coefs += error @ x * learning_rate\n",
    "intercept += error.sum() * learning_rate\n",
    "```\n",
    "\n",
    "### But why do we actually need a learning rate?\n",
    "\n",
    "#### An analogy to get you thinking\n",
    "Imagine you and 3 friends are trying to lift a table. The table weighs 100 pounds, but at the start each person is lifting only 10 pounds of the table's weight.\n",
    "\n",
    "That's 40 pounds in total, so everyone combined is 60 pounds short of holding the entire weight of the table; they haven't managed to move/lift it yet.\n",
    "\n",
    "If one person were to lift the remaining 60 pounds, they might manage to lift the table, but it would be lopsided (or maybe even flip!). We can compare this to modifying a single coefficient without using a learning rate. In this example, that one coefficient (person) would take the entire remaining weight of the table. \n",
    "\n",
    "If each one were to independently decide to lift the remaining 60 pounds, and they did so simultaneously, they would likely end up throwing the table into the air! We can compare this to over-fitting like _crazy_. Each coefficient is trying to do all of the work (lift the entire remaining weight of the table) on its own.\n",
    "\n",
    "The solution is for everyone to constantly make small adjustments until they smoothly/evenly lift the table off of the floor with each person ultimately taking about 25 pounds of the table's weight. \n",
    "\n",
    "To further the comparison to coefficients, if one side/corner of the table weighed more, then the person holding that side/corner would need to lift more. Lifting more weight would be like increasing the coefficient for that feature. Pushing this analogy even furhter; when all people are lifting the same amount, it's like having 4 features with perfect multicolinearity. But, unlike a single perfectly weighted coefficient, we can't all lift a 100 pound table on our own.\n",
    "\n",
    "#### Showing it with data:\n",
    "Below I train a simple model without using a learning rate, i.e. no small adustments.  \n",
    "This will result in **_extreme_** over-fitting as each coefficient will try to \"explain\" all of the change on its own.  \n",
    "I'll demonstrate this by showing two examples. \n",
    "\n",
    "Below, we train our very simple model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array of ones for our coefficients\n",
    "m = np.zeros_like(X_train[0])\n",
    "# The intercept\n",
    "b = 0\n",
    "# The number of oberservations\n",
    "n = X_train.shape[0]\n",
    "\n",
    "pred = X_train @ m + b\n",
    "error = (2/n) * (y_train - pred)\n",
    "\n",
    "d_m = error.T @ X_train\n",
    "d_b = error.sum()\n",
    "\n",
    "m += d_m\n",
    "b += d_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### First example - Using the Intercept as a predictor:\n",
    "The predicted `b` is equal to the mean of our target. The best (and only) prediction that can be made when there are no features (e.g. only an intercept and a target).  \n",
    "Pop-quiz: What will the r-squared be if we make predictions using only the intercept, `b`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "984.8134405883508 - The intercept that we've created\n",
      "984.8134405883509 - The mean of the target\n"
     ]
    }
   ],
   "source": [
    "print(b/2, \"- The intercept that we've created\")\n",
    "print(y_train.mean(), \"- The mean of the target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Second example - Using only one of our coefficients:\n",
    "The 8 coefficients without the intercept actually provide a reasonably accurate prediction for our target varianle. It makes sense that this works without the target variable because the adjustments that resulted in these coefficients were made without any knowledge that a y intercept might ever exist. They are the best fit given the available information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9303584913819554"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = X_test @ m\n",
    "r2_score(y_test, results/4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, that's what happens you don't take steps!\n",
    "\n",
    "Now let's go build a model that takes steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing a Stochastic Gradient Descent Model\n",
    "Now we can finally start creating our gradient descent model!\n",
    "\n",
    "I'm sure you can figure out most of what's going here but I'll point out a few things. We now have `learning_rate` and `max_iter` parameters. These weren't necessary for the normal equation because it's not iterative. Here we're taking steps; we need to know how big they are, and in case we're never going to converge, maybe it's just better to quit early and change your step size (and hopefully that allows you to converge... or just increase the number of iterations).\n",
    "\n",
    "There are many more parameters that I could add to this model for any number of reasons, but these will do for our purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StochasticGradientDescent:\n",
    "    \n",
    "    def __init__(self, learning_rate=0.05, max_iter=1_000):\n",
    "        self.l = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        \n",
    "    def __repr__(self):\n",
    "        parameters = (self.max_iter, self.l)\n",
    "        return \"LinearRegression(max_iter=%s, learning_rate=%s)\" % parameters\n",
    "    \n",
    "    def _gradient_descent(self, x, y):\n",
    "        # Remember that order matters for matrix multiplication but numpy\n",
    "        # allows some flexibility when multiplying (k, ) matrices which\n",
    "        # allows us to use 'mx + b' style notation\n",
    "        pred = self.coefs_ @ x + self.intercept\n",
    "        error = y - pred\n",
    "        self.coefs_ += error * x * self.l\n",
    "        self.intercept += error * self.l\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.intercept = 0\n",
    "        \n",
    "        # Create a numpy array of arbitrarily valued coefficients\n",
    "        # that will be updated as we iterate\n",
    "        self.coefs_ = np.ones_like(X_train[0])\n",
    "        \n",
    "        # We keep track of our r-squared \n",
    "        # When the r-squared in no longer decreasing above a certain\n",
    "        # rate. We've assume we've converged.\n",
    "        old_r2 = 0\n",
    "        \n",
    "        converged = False\n",
    "        \n",
    "        while not converged:\n",
    "            for row in zip(X_train, y_train):\n",
    "                \n",
    "                # Array of feature values for a single observation\n",
    "                x = row[0]\n",
    "                \n",
    "                # Target value for a single observation\n",
    "                y_obs = row[1]\n",
    "            \n",
    "                # Perform gradient descent and update \n",
    "                # our coefficients and intercept\n",
    "                self._gradient_descent(x, y_obs)\n",
    "            \n",
    "            # Create predictions\n",
    "            pred = (X_train @ self.coefs_) + self.intercept\n",
    "            \n",
    "            # Calculate the r-squared for those predictions\n",
    "            new_r2 = r2_score(pred, y_train)\n",
    "            \n",
    "            # Check the change in r-squared\n",
    "            if abs((old_r2) - (new_r2)) < 1e-15:\n",
    "                converged = True\n",
    "                \n",
    "            # Update the old r-squared\n",
    "            old_r2 = new_r2\n",
    "        \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        preds = (X_test @ self.coefs_) + self.intercept\n",
    "        return preds\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        preds = (X_test @ self.coefs_) + self.intercept\n",
    "        return r2_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.976366635108409"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "StochasticGradientDescent().fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Gradient Descent\n",
    "\n",
    "While stochastic gradient descent gets the job done, it's not the ideal model. Such frequent steps result in noisy updates for the coefficients and features on their way to the minima and convergence typically takessignificantly longer. One way to counteract this is via batch gradient descent. \n",
    "\n",
    "In essence, batch gradient descent looks over the entire set of training data and takes the average of the steps indicated. This provides a significantly more robust solution for calculating the step size.\n",
    "\n",
    "However, batch gradient descent is not without it's shortcominings. It uses the entire set of training data for each step. This becomes quite expensive as the number of iterations increases. \n",
    "\n",
    "This problem is solved via mini-bath gradient descent in which a random subset of observations are used for each iteration. Here, I've decided to take 10% of the observations. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniBatchGradientDescent:\n",
    "    \n",
    "    def __init__(self, max_iter=1_000, learning_rate=.05):\n",
    "        self.max_iter = max_iter\n",
    "        self.learning_rate = learning_rate\n",
    "    \n",
    "    def __repr__(self):\n",
    "        parameters = (self.max_iter, self.learning_rate)\n",
    "        return \"LinearRegression(max_iter=%s, learning_rate=%s)\" % parameters\n",
    "    \n",
    "    def _gradient_descent(self, X, y):\n",
    "        pred = X @ self.coefs_ + self.intercept\n",
    "        error = (1/self.batch_size) * (y - pred)\n",
    "\n",
    "        # error.T = (1 x n) \n",
    "        # X = (n x k) \n",
    "        # error x k -> (k x 1)\n",
    "        # These will become our updated coefficients\n",
    "        d_coefs = error.T @ X\n",
    "        d_intercept = error.sum()\n",
    "\n",
    "        self.coefs_ += self.learning_rate * d_coefs\n",
    "        self.intercept += self.learning_rate * d_intercept\n",
    "    \n",
    "    def fit(self, X_train, y_train):\n",
    "        self.coefs_ = np.zeros_like(X_train[0])\n",
    "        self.intercept = 0\n",
    "        \n",
    "        # Our number of observations\n",
    "        n = X_train.shape[0]\n",
    "        \n",
    "        # Calculating batch size from number of observations\n",
    "        self.batch_size = int(n/10)\n",
    "        \n",
    "        # Keeping track of number of iterations\n",
    "        i = 0\n",
    "        \n",
    "        old_r2 = 0\n",
    "        converged = False\n",
    "\n",
    "        while not converged:\n",
    "            batch_indices = np.random.choice(n, self.batch_size)\n",
    "            X_batch = X_train[batch_indices, :]\n",
    "            y_batch = y_train[batch_indices]\n",
    "            \n",
    "            # Perform gradient descent and update \n",
    "            # our coefficients and intercept\n",
    "            self._gradient_descent(X_batch, y_batch)\n",
    "\n",
    "            pred = (X_train @ self.coefs_) + self.intercept\n",
    "            new_r2 = r2_score(pred, y_train)\n",
    "            \n",
    "            i += 1\n",
    "            if abs((old_r2) - (new_r2)) < 1e-5:\n",
    "                converged = True\n",
    "            if i >= self.max_iter:\n",
    "                return self, print(\"Failed to converged after\", i, \"iterations.\")\n",
    "            old_r2 = new_r2\n",
    "            \n",
    "        return self\n",
    "        \n",
    "    def predict(self, X_test):\n",
    "        preds = (X_test @ self.coefs_) + self.intercept\n",
    "        return preds\n",
    "    \n",
    "    def score(self, X_test, y_test):\n",
    "        preds = (X_test @ self.coefs_) + self.intercept\n",
    "        return r2_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9538424013855976"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MiniBatchGradientDescent(max_iter=10_000).fit(X_train, y_train).score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not as fast or as reliable as sklearn's model, but an understanding of what is happening in the above will be a great asset if you ever need to understand how linear regression works under the hood.\n",
    "\n",
    "I hope you've enjoyed reading this as much as I went crazy writing it!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
